{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from utilities3 import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Adam import Adam\n",
    "from timeit import default_timer\n",
    "\n",
    "# import local modules from FNO2D.py\n",
    "import FNO2D\n",
    "\n",
    "\n",
    "import wandb\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "gc = pd.read_csv(\"Data/gc_samples_filtered.csv\", header=None)\n",
    "d = pd.read_csv(\"Data/d_samples_filtered.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in gc:  3207\n"
     ]
    }
   ],
   "source": [
    "# check number of samples in gc\n",
    "print(\"Number of samples in gc: \", len(gc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinates data from csv file\n",
    "coordinates = pd.read_csv(\"Data/coordinates_n\", header=None)\n",
    "\n",
    "# output mesh coordinates\n",
    "damage_x = pd.read_csv('Data/x_ver', header=None)\n",
    "damage_y = pd.read_csv('Data/y_ver', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ver shape:  (3918, 1)\n",
      "y_ver shape:  (3918, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_ver shape: \", damage_x.shape)\n",
    "print(\"y_ver shape: \", damage_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate damage_x and damage_y\n",
    "damage_xy = np.concatenate((damage_x, damage_y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gc shape:  (3207, 7610)\n",
      "d shape:  (3207, 3918)\n",
      "coordinates shape:  (7610, 2)\n",
      "damage_xy shape:  (3918, 2)\n"
     ]
    }
   ],
   "source": [
    "#  convert to torch tensor\n",
    "print(\"gc shape: \", gc.shape)\n",
    "print(\"d shape: \", d.shape)\n",
    "print(\"coordinates shape: \", coordinates.shape)\n",
    "print(\"damage_xy shape: \", damage_xy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create numpy array with shape (len(gc), gc.shape[1], 2)\n",
    "input_mesh = np.zeros((len(gc), gc.shape[1], 2))\n",
    "damage_mesh = np.zeros((len(gc), damage_xy.shape[0], 2))\n",
    "\n",
    "# for each sample in input_mesh, add the coordinates\n",
    "for i in range(len(gc)):\n",
    "    input_mesh[i, :, :] = coordinates\n",
    "    damage_mesh[i, :, :] = damage_xy[i, :]\n",
    "    \n",
    "# add gc to the last dimension of input_mesh to have shape of (len(gc), gc.shape[1], 3)\n",
    "input_data = np.concatenate((input_mesh, np.expand_dims(gc, axis=2)), axis=2)\n",
    "input_data = torch.from_numpy(input_data).float()\n",
    "\n",
    "damage_data = np.concatenate((damage_mesh, np.expand_dims(d, axis=2)), axis=2)\n",
    "damage_data = torch.from_numpy(damage_data).float()\n",
    "\n",
    "# damage_mesh = torch.from_numpy(damage_mesh).float()\n",
    "# input_mesh = torch.from_numpy(input_mesh).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# config for model\n",
    "step_size = 50\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 12\n",
    "width = 32\n",
    "\n",
    "# parameters\n",
    "# TEST_BATCH_SIZE = 8\n",
    "EPOCHS = 150\n",
    "\n",
    "# for optimizer\n",
    "LEARNING_RATE = 1e-3\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.999\n",
    "\n",
    "USE_CUDA = True\n",
    "\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split X data, for use in creating TensorDataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data, damage_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(torch.utils.data.TensorDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'FNO2D' from '/mnt/c/Users/Jin Yi/Documents/ArianaPHD/research/neural_operator/FNO2D.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(FNO2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, model_iphi):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    myloss = LpLoss(size_average=False)\n",
    "    for batch_idx, (data, damage) in enumerate(train_loader):\n",
    "        data, damage = data.to(device), damage.to(device)\n",
    "        damage_values = damage[:, :, 2]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, iphi=model_iphi, x_in = data[:, :, :2], x_out = damage[:, :, :2])\n",
    "        if len(damage_values) == BATCH_SIZE:\n",
    "            loss_data = myloss(output.view(BATCH_SIZE, -1), damage_values.view(BATCH_SIZE, -1))\n",
    "        else:\n",
    "            loss_data = myloss(output.view(len(damage_values), -1), damage_values.view(len(damage_values), -1))\n",
    "        # loss = loss_data + 0.000 * loss_reg\n",
    "        loss = loss_data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, valid_loader, model_iphi):\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    myloss = LpLoss(size_average=False)\n",
    "\n",
    "    data_list = []\n",
    "    output_list = []\n",
    "    damage_list = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, damage) in enumerate(valid_loader):\n",
    "            data, damage = data.to(device), damage.to(device)\n",
    "            damage_values = damage[:, :, 2]\n",
    "\n",
    "            output = model(data, iphi=model_iphi, x_in = data[:, :, :2], x_out = damage[:, :, :2])\n",
    "            if len(damage_values) == BATCH_SIZE:\n",
    "                loss_data = myloss(output.view(BATCH_SIZE, -1), damage_values.view(BATCH_SIZE, -1))\n",
    "            else:\n",
    "                loss_data = myloss(output.view(len(damage_values), -1), damage_values.view(len(damage_values), -1))\n",
    "            loss = loss_data\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            data_list.append(data.cpu().numpy())\n",
    "            output_list.append(output.cpu().numpy())\n",
    "            damage_list.append(damage.cpu().numpy())\n",
    "\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "\n",
    "    return valid_loss, data_list, output_list, damage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1482689 63746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marianaqyp\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Jin Yi/Documents/ArianaPHD/research/neural_operator/wandb/run-20230122_003946-bq7sc8to</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/arianaqyp/pca_znet_lifted-20230122/runs/bq7sc8to\" target=\"_blank\">20230122_0039</a></strong> to <a href=\"https://wandb.ai/arianaqyp/pca_znet_lifted-20230122\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinyi12/miniconda3/envs/phasefield/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 1.3645655, Valid Loss: 0.9089653, LR: 0.0010000\n",
      "Validation loss decreased (inf --> 0.908965).  Saving model ...\n",
      "Saved model at epoch 1\n",
      "Epoch: 002, Train Loss: 0.9087615, Valid Loss: 0.9081031, LR: 0.0010000\n",
      "Validation loss decreased (0.908965 --> 0.908103).  Saving model ...\n",
      "Saved model at epoch 2\n",
      "Epoch: 003, Train Loss: 0.9083766, Valid Loss: 0.9079667, LR: 0.0010000\n",
      "Validation loss decreased (0.908103 --> 0.907967).  Saving model ...\n",
      "Saved model at epoch 3\n",
      "Epoch: 004, Train Loss: 0.9082507, Valid Loss: 0.9079933, LR: 0.0010000\n",
      "Epoch: 005, Train Loss: 0.9081765, Valid Loss: 0.9080418, LR: 0.0010000\n",
      "Epoch: 006, Train Loss: 0.9081563, Valid Loss: 0.9078908, LR: 0.0010000\n",
      "Validation loss decreased (0.907967 --> 0.907891).  Saving model ...\n",
      "Saved model at epoch 6\n",
      "Epoch: 007, Train Loss: 0.9081881, Valid Loss: 0.9079175, LR: 0.0010000\n",
      "Epoch: 008, Train Loss: 0.9081116, Valid Loss: 0.9084197, LR: 0.0010000\n",
      "Epoch: 009, Train Loss: 0.9080413, Valid Loss: 0.9080461, LR: 0.0010000\n",
      "Epoch: 010, Train Loss: 0.9081132, Valid Loss: 0.9078882, LR: 0.0010000\n",
      "Validation loss decreased (0.907891 --> 0.907888).  Saving model ...\n",
      "Saved model at epoch 10\n",
      "Epoch: 011, Train Loss: 2.4784730, Valid Loss: 0.9661903, LR: 0.0010000\n",
      "Epoch: 012, Train Loss: 0.9162471, Valid Loss: 0.9093025, LR: 0.0010000\n",
      "Epoch: 013, Train Loss: 0.9085952, Valid Loss: 0.9082357, LR: 0.0010000\n",
      "Epoch: 014, Train Loss: 0.9086192, Valid Loss: 0.9131937, LR: 0.0010000\n",
      "Epoch: 015, Train Loss: 0.9084607, Valid Loss: 0.9121792, LR: 0.0010000\n",
      "Epoch: 016, Train Loss: 0.9088740, Valid Loss: 0.9079738, LR: 0.0010000\n",
      "Epoch: 017, Train Loss: 0.9091927, Valid Loss: 0.9097377, LR: 0.0010000\n",
      "Epoch: 018, Train Loss: 0.9093556, Valid Loss: 0.9085861, LR: 0.0010000\n",
      "Epoch: 019, Train Loss: 0.9092998, Valid Loss: 0.9079433, LR: 0.0010000\n",
      "Epoch: 020, Train Loss: 0.9090042, Valid Loss: 0.9109454, LR: 0.0010000\n",
      "Epoch: 021, Train Loss: 0.9088831, Valid Loss: 0.9083613, LR: 0.0010000\n",
      "Epoch: 022, Train Loss: 0.9083710, Valid Loss: 0.9082201, LR: 0.0010000\n",
      "Epoch: 023, Train Loss: 0.9083670, Valid Loss: 0.9081275, LR: 0.0010000\n",
      "Epoch: 024, Train Loss: 0.9083194, Valid Loss: 0.9084057, LR: 0.0010000\n",
      "Epoch: 025, Train Loss: 0.9163931, Valid Loss: 0.9080476, LR: 0.0010000\n",
      "Epoch: 026, Train Loss: 0.9081987, Valid Loss: 0.9078832, LR: 0.0010000\n",
      "Validation loss decreased (0.907888 --> 0.907883).  Saving model ...\n",
      "Saved model at epoch 26\n",
      "Epoch: 027, Train Loss: 0.9085819, Valid Loss: 0.9078877, LR: 0.0010000\n",
      "Epoch: 028, Train Loss: 0.9080413, Valid Loss: 0.9079833, LR: 0.0010000\n",
      "Epoch: 029, Train Loss: 0.9080488, Valid Loss: 0.9080300, LR: 0.0010000\n",
      "Epoch: 030, Train Loss: 0.9080159, Valid Loss: 0.9078968, LR: 0.0010000\n",
      "Epoch: 031, Train Loss: 0.9080079, Valid Loss: 0.9078915, LR: 0.0010000\n",
      "Epoch: 032, Train Loss: 0.9080584, Valid Loss: 0.9079209, LR: 0.0010000\n",
      "Epoch: 033, Train Loss: 1.4047612, Valid Loss: 0.9101985, LR: 0.0010000\n",
      "Epoch: 034, Train Loss: 0.9095957, Valid Loss: 0.9080839, LR: 0.0010000\n",
      "Epoch: 035, Train Loss: 0.9082274, Valid Loss: 0.9088236, LR: 0.0010000\n",
      "Epoch: 036, Train Loss: 0.9084079, Valid Loss: 0.9080547, LR: 0.0010000\n",
      "Epoch: 037, Train Loss: 0.9086679, Valid Loss: 0.9079919, LR: 0.0010000\n",
      "Epoch: 038, Train Loss: 0.9085643, Valid Loss: 0.9086413, LR: 0.0010000\n",
      "Epoch: 039, Train Loss: 0.9091480, Valid Loss: 0.9088672, LR: 0.0010000\n",
      "Epoch: 040, Train Loss: 0.9087991, Valid Loss: 0.9085018, LR: 0.0010000\n",
      "Epoch: 041, Train Loss: 0.9086432, Valid Loss: 0.9096013, LR: 0.0010000\n",
      "Epoch: 042, Train Loss: 0.9085241, Valid Loss: 0.9078579, LR: 0.0010000\n",
      "Validation loss decreased (0.907883 --> 0.907858).  Saving model ...\n",
      "Saved model at epoch 42\n",
      "Epoch: 043, Train Loss: 0.9084241, Valid Loss: 0.9090781, LR: 0.0010000\n",
      "Epoch: 044, Train Loss: 0.9092799, Valid Loss: 0.9084787, LR: 0.0010000\n",
      "Epoch: 045, Train Loss: 0.9081352, Valid Loss: 0.9078530, LR: 0.0010000\n",
      "Validation loss decreased (0.907858 --> 0.907853).  Saving model ...\n",
      "Saved model at epoch 45\n",
      "Epoch: 046, Train Loss: 0.9083043, Valid Loss: 0.9091320, LR: 0.0010000\n",
      "Epoch: 047, Train Loss: 1.0636463, Valid Loss: 0.9101276, LR: 0.0010000\n",
      "Epoch: 048, Train Loss: 0.9081986, Valid Loss: 0.9081350, LR: 0.0010000\n",
      "Epoch: 049, Train Loss: 0.9081130, Valid Loss: 0.9078690, LR: 0.0010000\n",
      "Epoch: 050, Train Loss: 0.9079618, Valid Loss: 0.9078562, LR: 0.0005000\n",
      "Epoch: 051, Train Loss: 0.9079872, Valid Loss: 0.9078982, LR: 0.0005000\n"
     ]
    }
   ],
   "source": [
    "n_list = [32]\n",
    "\n",
    "INPUT_CHANNELS = 3\n",
    "OUTPUT_CHANNELS = 1\n",
    "\n",
    "for i in range(len(n_list)):\n",
    "\n",
    "    min_valid_loss = np.inf\n",
    "\n",
    "    width = n_list[i]\n",
    "\n",
    "    today = datetime.datetime.now()\n",
    "\n",
    "    model = FNO2D.FNO2d(modes, modes, width=width, in_channels=INPUT_CHANNELS, out_channels=OUTPUT_CHANNELS).cuda()\n",
    "    model_iphi = FNO2D.IPHI(width=width).cuda()\n",
    "\n",
    "    print(count_params(model), count_params(model_iphi))\n",
    "\n",
    "    params = list(model.parameters()) + list(model_iphi.parameters())\n",
    "    optimizer = Adam(params, lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "\n",
    "    data_list = []\n",
    "    output_list = []\n",
    "\n",
    "    wandb.init(\n",
    "        anonymous='allow', project=\"pca_znet_lifted-\" + today.strftime('%Y%m%d'), name= today.strftime('%Y%m%d_%H%M'),\n",
    "            config={\n",
    "                \"epochs\": EPOCHS,\n",
    "                 \"optimizer\": 'Adam',\n",
    "                \"batch_size\": BATCH_SIZE, 'lr': LEARNING_RATE,\n",
    "                'step_size': step_size, 'gamma': gamma,\n",
    "                'width': width,\n",
    "                'modes': modes,\n",
    "                'loss ': 'L2Loss',\n",
    "                'activation func': 'GELU',\n",
    "                'lr decay': 'steplr, gamma=0.5',\n",
    "                'in_channels': INPUT_CHANNELS, 'out_channels': OUTPUT_CHANNELS,\n",
    "                'architecture': 'FNO2D Original',\n",
    "                }\n",
    "        )\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        scheduler.step()\n",
    "\n",
    "        # get current learning rate\n",
    "        curr_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "        train_loss = train(model, device, train_loader, optimizer, model_iphi)\n",
    "        valid_loss, data_list, output_list, damage_list = validate(model, device, test_loader, model_iphi)\n",
    "        print('Epoch: {:03d}, Train Loss: {:.7f}, Valid Loss: {:.7f}, LR: {:.7f}'.format(epoch, train_loss, valid_loss, curr_lr))\n",
    "        wandb.log({'train_loss': train_loss, 'valid_loss': valid_loss})\n",
    "        train_loss_list.append(train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "\n",
    "        if valid_loss < min_valid_loss:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(min_valid_loss, valid_loss))\n",
    "            min_valid_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'Model/model.pt')\n",
    "            torch.save(model_iphi.state_dict(), 'Model/model_iphi.pt')\n",
    "            print('Saved model at epoch {}'.format(epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cuda cache and memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 5.0 GB\n",
      "Cached:    5.3 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinyi12/miniconda3/envs/phasefield/lib/python3.8/site-packages/torch/cuda/memory.py:384: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phasefield",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb9419f644557548447ecb0f1119aef2567eb0b9ec92744eeb7ce809f1fc1aa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
