{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from utilities3 import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Adam import Adam\n",
    "from timeit import default_timer\n",
    "\n",
    "# import local modules from FNO2D.py\n",
    "import FNO2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "gc = pd.read_csv(\"Data/gc_samples_filtered.csv\", header=None)\n",
    "d = pd.read_csv(\"Data/d_samples_filtered.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in gc:  3207\n"
     ]
    }
   ],
   "source": [
    "# check number of samples in gc\n",
    "print(\"Number of samples in gc: \", len(gc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinates data from csv file\n",
    "coordinates = pd.read_csv(\"Data/coordinates_n\", header=None)\n",
    "\n",
    "# output mesh coordinates\n",
    "damage_x = pd.read_csv('Data/x_ver', header=None)\n",
    "damage_y = pd.read_csv('Data/y_ver', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ver shape:  (3918, 1)\n",
      "y_ver shape:  (3918, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_ver shape: \", damage_x.shape)\n",
    "print(\"y_ver shape: \", damage_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate damage_x and damage_y\n",
    "damage_xy = np.concatenate((damage_x, damage_y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gc shape:  (3207, 7610)\n",
      "d shape:  (3207, 3918)\n",
      "coordinates shape:  (7610, 2)\n",
      "damage_xy shape:  (3918, 2)\n"
     ]
    }
   ],
   "source": [
    "#  convert to torch tensor\n",
    "print(\"gc shape: \", gc.shape)\n",
    "print(\"d shape: \", d.shape)\n",
    "print(\"coordinates shape: \", coordinates.shape)\n",
    "print(\"damage_xy shape: \", damage_xy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create numpy array with shape (len(gc), gc.shape[1], 2)\n",
    "input_mesh = np.zeros((len(gc), gc.shape[1], 2))\n",
    "damage_mesh = np.zeros((len(gc), damage_xy.shape[0], 2))\n",
    "\n",
    "# for each sample in input_mesh, add the coordinates\n",
    "for i in range(len(gc)):\n",
    "    input_mesh[i, :, :] = coordinates\n",
    "    damage_mesh[i, :, :] = damage_xy[i, :]\n",
    "    \n",
    "# add gc to the last dimension of input_mesh to have shape of (len(gc), gc.shape[1], 3)\n",
    "input_data = np.concatenate((input_mesh, np.expand_dims(gc, axis=2)), axis=2)\n",
    "input_data = torch.from_numpy(input_data).float()\n",
    "\n",
    "damage_data = np.concatenate((damage_mesh, np.expand_dims(d, axis=2)), axis=2)\n",
    "damage_data = torch.from_numpy(damage_data).float()\n",
    "\n",
    "# damage_mesh = torch.from_numpy(damage_mesh).float()\n",
    "# input_mesh = torch.from_numpy(input_mesh).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config for model\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 501\n",
    "step_size = 50\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 12\n",
    "width = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split X data, for use in creating TensorDataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data, damage_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(torch.utils.data.TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'FNO2D' from '/mnt/c/Users/Jin Yi/Documents/ArianaPHD/research/neural_operator/FNO2D.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(FNO2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNO2D.FNO2d(modes, modes, width=width, in_channels=3, out_channels=1).cuda()\n",
    "model_iphi = FNO2D.IPHI(width=width).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1482689 63746\n"
     ]
    }
   ],
   "source": [
    "print(count_params(model), count_params(model_iphi))\n",
    "\n",
    "params = list(model.parameters()) + list(model_iphi.parameters())\n",
    "optimizer = Adam(params, lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2565"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3207, 7610, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[:, :, :2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 23.459415493067354 1.815806576615421 0.9083900696763368\n",
      "1 23.326877016108483 0.9085984008818807 0.9081332464455816\n",
      "2 23.375878255348653 0.908978809297201 0.9096143048871714\n",
      "3 23.369796778075397 0.9085510887830114 0.9088523183656259\n"
     ]
    }
   ],
   "source": [
    "myloss = LpLoss(size_average=False)\n",
    "N_sample = 1000 # number of samples for regularization\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    t1 = default_timer()\n",
    "    train_l2 = 0\n",
    "    train_reg = 0\n",
    "    for data, damage in train_loader:\n",
    "        damage, data = damage.cuda(), data.cuda()\n",
    "        # samples_x = torch.rand(batch_size, N_sample, 3).cuda() * 3 -1\n",
    "\n",
    "        damage_values = damage[:, :, 2]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data, iphi=model_iphi, x_in = data[:, :, :2], x_out = damage[:, :, :2])\n",
    "        # print(out.shape)\n",
    "        # samples_xi = model_iphi(samples_x)\n",
    "\n",
    "        # if not last batch, use batch_size, else use the remainder\n",
    "        if len(damage_values) == batch_size:\n",
    "            loss_data = myloss(out.view(batch_size, -1), damage_values.view(batch_size, -1))\n",
    "        else:\n",
    "            loss_data = myloss(out.view(len(damage_values), -1), damage_values.view(len(damage_values), -1))\n",
    "        # loss_reg = myloss(samples_xi, samples_x)\n",
    "        # loss = loss_data + 0.000 * loss_reg\n",
    "        loss = loss_data + 0.000\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_l2 += loss_data.item()\n",
    "        # train_reg += loss_reg.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, damage in test_loader:\n",
    "            data, damage = data.cuda(), damage.cuda()\n",
    "            # out = model(mesh, iphi=model_iphi)\n",
    "            damage_values_test = damage[:, :, 2]\n",
    "            out = model(data, iphi=model_iphi, x_in = data[:, :, :2], x_out = damage[:, :, :2])\n",
    "            # if not last batch, use batch_size, else use the remainder\n",
    "            if len(damage_values_test) == batch_size:\n",
    "                test_l2 += myloss(out.view(batch_size, -1), damage_values_test.view(batch_size, -1)).item()\n",
    "            else:\n",
    "                test_l2 += myloss(out.view(len(damage_values_test), -1), damage_values_test.view(len(damage_values_test), -1)).item()\n",
    "\n",
    "    train_l2 /= X_train.shape[0]\n",
    "    # train_reg /= X_train.shape[0]\n",
    "    test_l2 /= X_test.shape[0]\n",
    "\n",
    "    t2 = default_timer()\n",
    "    print(ep, t2 - t1, train_l2, test_l2)\n",
    "\n",
    "    if ep%100==0:\n",
    "        XY = data[-1].squeeze().detach().cpu().numpy()\n",
    "        truth = damage[-1].squeeze().detach().cpu().numpy()\n",
    "        pred = out[-1].squeeze().detach().cpu().numpy()\n",
    "\n",
    "        # lims = dict(cmap='RdBu_r', vmin=truth.min(), vmax=truth.max())\n",
    "        # fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "        # ax[0].scatter(XY[:, 0], XY[:, 1], 100, truth, edgecolor='w', lw=0.1, **lims)\n",
    "        # ax[1].scatter(XY[:, 0], XY[:, 1], 100, pred, edgecolor='w', lw=0.1, **lims)\n",
    "        # ax[2].scatter(XY[:, 0], XY[:, 1], 100, truth - pred, edgecolor='w', lw=0.1, **lims)\n",
    "        # fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cuda cache and memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 5.0 GB\n",
      "Cached:    5.3 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinyi12/miniconda3/envs/phasefield/lib/python3.8/site-packages/torch/cuda/memory.py:384: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phasefield",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb9419f644557548447ecb0f1119aef2567eb0b9ec92744eeb7ce809f1fc1aa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
