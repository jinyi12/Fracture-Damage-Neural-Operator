{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import dadaptation\n",
    "import random\n",
    "\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from utilities3 import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Adam import Adam\n",
    "from timeit import default_timer\n",
    "from collections import OrderedDict\n",
    "\n",
    "# import local modules from FNO2D.py\n",
    "import FNO2D\n",
    "\n",
    "\n",
    "import wandb\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "PROJECT_NAME = 'FNO2D'\n",
    "\n",
    "# Set the random seeds to improve reproducibility by removing stochasticity\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False # Force cuDNN to use a consistent convolution algorithm\n",
    "    torch.backends.cudnn.deterministic = True # Force cuDNN to use deterministic algorithms if available\n",
    "    torch.use_deterministic_algorithms(True) # Force torch to use deterministic algorithms if available\n",
    "\n",
    "set_seeds(0)\n",
    "\n",
    "# for deterministic pytorch algorithms, enable reproducibility.\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG']= \":4096:8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with wandb.init(project=\"FNO2D\", entity=\"jyyresearch\", job_type=\"get-raw-data\") as run:\n",
    "#     raw_data = run.use_artifact('jyyresearch/FNO2D/fracture-damage-raw-data:latest', type='raw_data')\n",
    "#     raw_data_dir = raw_data.download()\n",
    "#     gc_data = h5py.File(raw_data_dir + \"/gc_data\", \"r\")\n",
    "#     damage_data = h5py.File(raw_data_dir + \"/damage_data\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'train_val_split': [0.80, 0.20], # These must sum to 1.0\n",
    "    'batch_size' : 16, # Num samples to average over for gradient updates\n",
    "    'EPOCHS' : 200, # Num times to iterate over the entire dataset\n",
    "    'LEARNING_RATE' : 1e-4, # Learning rate for the optimizer\n",
    "    'BETA1' : 0.9, # Beta1 parameter for the Adam optimizer\n",
    "    'BETA2' : 0.999, # Beta2 parameter for the Adam optimizer\n",
    "    'WEIGHT_DECAY' : 1e-4, # Weight decay parameter for the Adam optimizer\n",
    "    'accum_iter': 16, # iterations to accumulate gradients\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some helper functions for transforming numpy to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert numpy arrays to tensor arrays\n",
    "    \"\"\"\n",
    "    def __init__(self, device=None):\n",
    "        if device is None:\n",
    "            device = \"cpu\"\n",
    "        self.device = device\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        if self.device == \"cpu\":\n",
    "            return torch.from_numpy(data)\n",
    "        else:\n",
    "            # to overlap data transfers with computation, use non_blocking=True\n",
    "            return torch.from_numpy(data).to(self.device, non_blocking=True, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(transform_dict):\n",
    "    \"\"\"\n",
    "    Given a dictionary of transform parameters, return a list of class instances for each transform\n",
    "    Arguments:\n",
    "        transform_dict (OrderedDict) with optional keys:\n",
    "            ToTensor (dict) if present, requires the 'device' key that indicates the PyTorch device\n",
    "    Returns:\n",
    "        composed_transforms (PyTorch composed transform class) containing the requested transform steps in order\n",
    "    \"\"\"\n",
    "    transform_functions = []\n",
    "    for key in transform_dict.keys():\n",
    "        if key=='ToTensor': # Convert array to a PyTorch Tensor\n",
    "            transform_functions.append(ToTensor(\n",
    "                transform_dict[key]['device']\n",
    "            ))\n",
    "        \n",
    "    composed_transforms = transforms.Compose(transform_functions)\n",
    "    return composed_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a torch dataset\n",
    "class FractureDamageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, gc_data, damage_data, transform=None):\n",
    "        self.gc_data = gc_data\n",
    "        self.damage_data = damage_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gc_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        gc = self.gc_data[idx] \n",
    "        damage = self.damage_data[idx]\n",
    "        if self.transform:\n",
    "            gc = self.transform(gc)\n",
    "            damage = self.transform(damage)\n",
    "        return gc, damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split_artifact(run, train_rows, val_rows):\n",
    "    \"\"\"\n",
    "    Creates a w&b artifact that contains the train and validation rows of the raw data\n",
    "        run (wandb run) returned from wandb.init()\n",
    "        train_rows (list of ints) indices that reference the training rows in the raw_data\n",
    "        val_rows (list of ints) indices that reference the validation rows in the raw_data\n",
    "    \"\"\"\n",
    "    split_artifact = wandb.Artifact(\n",
    "        'data-splits', type='dataset',\n",
    "        description='Train, validation, test dataset splits')\n",
    "\n",
    "    # Our data split artifact will only store index references to the original dataset to save space\n",
    "    split_artifact.add(wandb.Table(\n",
    "        data=pd.DataFrame(train_rows, columns=['indices'])), 'train-data')\n",
    "\n",
    "    split_artifact.add(wandb.Table(\n",
    "        columns=['source'],\n",
    "        data=pd.DataFrame(val_rows, columns=['indices'])), 'val-data')\n",
    "\n",
    "    run.log_artifact(split_artifact)\n",
    "\n",
    "\n",
    "def make_loaders(config, gc_data, damage_data):\n",
    "    \"\"\"\n",
    "    Makes data loaders using a artifact containing the dataset splits (created using the make_split_artifact() function)\n",
    "    The function assumes that you have created a data-splits artifact and a data-transforms artifact\n",
    "    Arguments:\n",
    "        config [dict] containing keys:\n",
    "            batch_size (int) amount of rows (i.e. data instances) to be delivered in a single batch\n",
    "    Returns:\n",
    "        train_loader (PyTorch DataLoader) containing the training data\n",
    "        val_loader (PyTorch DataLoader) containing the validation data\n",
    "    \"\"\"\n",
    "    with wandb.init(project=PROJECT_NAME, job_type='package-data', config=config) as run:\n",
    "        # Load transforms\n",
    "        transform_dir = run.use_artifact('data-transforms:latest').download()\n",
    "        transform_dict = json.load(open(os.path.join(transform_dir, 'transforms.txt')), object_pairs_hook=OrderedDict)\n",
    "        composed_transforms = get_transforms(transform_dict)\n",
    "\n",
    "        split_artifact = run.use_artifact('data-splits:latest')\n",
    "\n",
    "        # Load splits\n",
    "        # its a wandb.Table data type so we can use the get() method\n",
    "        train_rows = split_artifact.get('train-data').get_column('indices', convert_to='numpy')\n",
    "        val_rows = split_artifact.get('val-data').get_column('indices', convert_to='numpy')\n",
    "\n",
    "        # Reformat data to (inputs, labels)\n",
    "        train_loader = DataLoader(FractureDamageDataset(\n",
    "            gc_data[train_rows], damage_data[train_rows], transform=composed_transforms),\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        val_loader = DataLoader(FractureDamageDataset(\n",
    "            gc_data[val_rows], damage_data[val_rows], transform=composed_transforms),\n",
    "            batch_size=config['batch_size'],\n",
    "            batch_sampler=None,\n",
    "            shuffle=False,\n",
    "            num_workers=0)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the raw data by downloading it into a directory, load the raw data and create indices for train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjyyresearch\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Jin Yi/Documents/ArianaPHD/research/neural_operator/wandb/run-20230228_231400-1jw0mg95</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/1jw0mg95\" target=\"_blank\">fragrant-wave-234</a></strong> to <a href=\"https://wandb.ai/jyyresearch/FNO2D\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact fracture-damage-raw-data:latest, 729.06MB. 2 files... Done. 0:0:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b1d3eb39d34ae2be563750955483e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fragrant-wave-234</strong>: <a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/1jw0mg95\" target=\"_blank\">https://wandb.ai/jyyresearch/FNO2D/runs/1jw0mg95</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230228_231400-1jw0mg95/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=PROJECT_NAME, job_type=\"split-data\", config=config) as run:\n",
    "\n",
    "    # Define raw data splits\n",
    "    raw_data = run.use_artifact('jyyresearch/FNO2D/fracture-damage-raw-data:latest', type='raw_data')\n",
    "\n",
    "    raw_data_dir = raw_data.download()\n",
    "\n",
    "    # read in the h5 files\n",
    "    gc_data = h5py.File(os.path.join(raw_data_dir, 'gc_data'), 'r')['gc_data'][:]\n",
    "    damage_data = h5py.File(os.path.join(raw_data_dir, 'damage_data'), 'r')['damage_data'][:]\n",
    "\n",
    "    # train test split of gc_data and damage_data. Obtain the respective indices\n",
    "    train_val_split = config['train_val_split']\n",
    "    train_val_indices = np.split(np.random.permutation(len(gc_data)), [int(train_val_split[0]*len(gc_data))])\n",
    "    \n",
    "    make_split_artifact(run, train_val_indices[0], train_val_indices[1])\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make our dataloaders using our uploaded composed transform, and our train and val indices, and also our raw_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Jin Yi/Documents/ArianaPHD/research/neural_operator/wandb/run-20230228_231423-2c574e0c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/2c574e0c\" target=\"_blank\">winter-smoke-235</a></strong> to <a href=\"https://wandb.ai/jyyresearch/FNO2D\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8149302426414cbc91e898b9fade6a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">winter-smoke-235</strong>: <a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/2c574e0c\" target=\"_blank\">https://wandb.ai/jyyresearch/FNO2D/runs/2c574e0c</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230228_231423-2c574e0c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Jin Yi/Documents/ArianaPHD/research/neural_operator/wandb/run-20230228_231435-34m8uh0c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/34m8uh0c\" target=\"_blank\">dulcet-sky-236</a></strong> to <a href=\"https://wandb.ai/jyyresearch/FNO2D\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a25e1b385b48c6ab36ddf4450bae52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dulcet-sky-236</strong>: <a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/34m8uh0c\" target=\"_blank\">https://wandb.ai/jyyresearch/FNO2D/runs/34m8uh0c</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230228_231435-34m8uh0c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define an initial set of transforms that we think will be useful\n",
    "with wandb.init(project=PROJECT_NAME, job_type='define-transforms', config=config) as run:\n",
    "    transform_dict = OrderedDict()\n",
    "    transform_dict['ToTensor'] = {\n",
    "        'device': DEVICE\n",
    "    }\n",
    "    # Include an operational index to verify the order\n",
    "    for key_idx, key in enumerate(transform_dict.keys()):\n",
    "        transform_dict[key]['order'] = key_idx\n",
    "    # Create an artifact for logging the transforms\n",
    "    data_transform_artifact = wandb.Artifact(\n",
    "        'data-transforms', type='parameters',\n",
    "        description='Data preprocessing functions and parameters.',\n",
    "        metadata=transform_dict) # Optional for viewing on the web app; the data is also stored in the txt file below\n",
    "    # Log the transforms in JSON format\n",
    "    with data_transform_artifact.new_file('transforms.txt') as f:\n",
    "        f.write(json.dumps(transform_dict, indent=4))\n",
    "    run.log_artifact(data_transform_artifact)\n",
    "\n",
    "config.update(transform_dict)\n",
    "\n",
    "train_loader, val_loader = make_loaders(config, gc_data=gc_data, damage_data=damage_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config for model\n",
    "step_size = 50\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 12\n",
    "s = modes * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, model_iphi, config):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    myloss = LpLoss(size_average=False)\n",
    "\n",
    "    accum_iter = config['accum_iter']\n",
    "\n",
    "    for batch_idx, (data, damage) in enumerate(train_loader):        \n",
    "\n",
    "        # HEAVISIDE WEIGHT FUNCTION\n",
    "        # find index of values > 0.3\n",
    "        # w_batch_index = np.apply_along_axis(lambda x: x > 0.3, 1, damage[:, :, 2].numpy())\n",
    "        # weights_norm = np.where(w_batch_index, (0.8/np.sum(w_batch_index, axis=1))[:, np.newaxis], (0.2/np.sum(~w_batch_index, axis=1))[:, np.newaxis])\n",
    "        # weights_norm = torch.from_numpy(weights_norm).float().cuda()   # to tensor\n",
    "\n",
    "        \n",
    "        \n",
    "        data, damage = data.to(device), damage.to(device)\n",
    "        damage_values = damage[:, :, 2]\n",
    "\n",
    "        output = model(data, iphi=model_iphi, x_in = data[:, :, :2], x_out = damage[:, :, :2])\n",
    "\n",
    "        if len(damage_values) == config['batch_size']:\n",
    "            loss_data = myloss(output.view(config['batch_size'], -1), damage_values.view(config['batch_size'], -1))\n",
    "        else:\n",
    "            loss_data = myloss(output.view(len(damage_values), -1), damage_values.view(len(damage_values), -1))\n",
    "        # loss = loss_data + 0.000 * loss_reg\n",
    "        loss = loss_data/accum_iter\n",
    "        loss.backward()\n",
    "\n",
    "        # perform gradient accumulation\n",
    "        if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == len(train_loader)):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, valid_loader, model_iphi, config):\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "\n",
    "    data_list = []\n",
    "    output_list = []\n",
    "    damage_list = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        myloss = LpLoss(size_average=False)\n",
    "        for batch_idx, (data, damage) in enumerate(valid_loader):\n",
    "            data, damage = data.to(device), damage.to(device)\n",
    "            damage_values = damage[:, :, 2]\n",
    "\n",
    "            # HEAVISIDE WEIGHT FUNCTION\n",
    "            # # find index of values > 0.3\n",
    "            # w_batch_index = np.apply_along_axis(lambda x: x > 0.3, 1, damage[:, :, 2].cpu().numpy())\n",
    "            # weights_norm = np.where(w_batch_index, (0.8/np.sum(w_batch_index, axis=1))[:, np.newaxis], (0.2/np.sum(~w_batch_index, axis=1))[:, np.newaxis])\n",
    "            # weights_norm = torch.from_numpy(weights_norm).float().cuda()\n",
    "\n",
    "            # myloss = MSELoss_weighted(weights_tensor = weights_norm)\n",
    "\n",
    "            output = model(data, iphi=model_iphi, x_in = data[:, :, :2], x_out = damage[:, :, :2])\n",
    "            if len(damage_values) == config['batch_size']:\n",
    "                loss_data = myloss(output.view(config['batch_size'], -1), damage_values.view(config['batch_size'], -1))\n",
    "            else:\n",
    "                loss_data = myloss(output.view(len(damage_values), -1), damage_values.view(len(damage_values), -1))\n",
    "            loss = loss_data + 0\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            data_list.append(data.cpu().numpy())\n",
    "            output_list.append(output.cpu().numpy())\n",
    "            damage_list.append(damage.cpu().numpy())\n",
    "\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "\n",
    "    return valid_loss, data_list, output_list, damage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'FNO2D' from '/mnt/c/Users/Jin Yi/Documents/ArianaPHD/research/neural_operator/FNO2D.py'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(FNO2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1487041 63746\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Jin Yi/Documents/ArianaPHD/research/neural_operator/wandb/run-20230228_231456-2h55d0r4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/2h55d0r4\" target=\"_blank\">20230228_2314</a></strong> to <a href=\"https://wandb.ai/jyyresearch/FNO2D\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.0045147, Valid Loss: 0.0579973, LR: 0.0001000\n",
      "Validation loss decreased (inf --> 0.057997).  Saving model ...\n",
      "Saved model at epoch 1\n",
      "Epoch: 002, Train Loss: 0.0035510, Valid Loss: 0.0550044, LR: 0.0001000\n",
      "Validation loss decreased (0.057997 --> 0.055004).  Saving model ...\n",
      "Saved model at epoch 2\n",
      "Epoch: 003, Train Loss: 0.0033634, Valid Loss: 0.0528600, LR: 0.0001000\n",
      "Validation loss decreased (0.055004 --> 0.052860).  Saving model ...\n",
      "Saved model at epoch 3\n",
      "Epoch: 004, Train Loss: 0.0032657, Valid Loss: 0.0517323, LR: 0.0001000\n",
      "Validation loss decreased (0.052860 --> 0.051732).  Saving model ...\n",
      "Saved model at epoch 4\n",
      "Epoch: 005, Train Loss: 0.0031785, Valid Loss: 0.0498312, LR: 0.0001000\n",
      "Validation loss decreased (0.051732 --> 0.049831).  Saving model ...\n",
      "Saved model at epoch 5\n",
      "Epoch: 006, Train Loss: 0.0030543, Valid Loss: 0.0478706, LR: 0.0001000\n",
      "Validation loss decreased (0.049831 --> 0.047871).  Saving model ...\n",
      "Saved model at epoch 6\n",
      "Epoch: 007, Train Loss: 0.0029270, Valid Loss: 0.0461424, LR: 0.0001000\n",
      "Validation loss decreased (0.047871 --> 0.046142).  Saving model ...\n",
      "Saved model at epoch 7\n",
      "Epoch: 008, Train Loss: 0.0028608, Valid Loss: 0.0467578, LR: 0.0001000\n",
      "Epoch: 009, Train Loss: 0.0028608, Valid Loss: 0.0435391, LR: 0.0001000\n",
      "Validation loss decreased (0.046142 --> 0.043539).  Saving model ...\n",
      "Saved model at epoch 9\n",
      "Epoch: 010, Train Loss: 0.0026617, Valid Loss: 0.0428577, LR: 0.0001000\n",
      "Validation loss decreased (0.043539 --> 0.042858).  Saving model ...\n",
      "Saved model at epoch 10\n",
      "Epoch: 011, Train Loss: 0.0025596, Valid Loss: 0.0407431, LR: 0.0001000\n",
      "Validation loss decreased (0.042858 --> 0.040743).  Saving model ...\n",
      "Saved model at epoch 11\n",
      "Epoch: 012, Train Loss: 0.0024803, Valid Loss: 0.0399507, LR: 0.0001000\n",
      "Validation loss decreased (0.040743 --> 0.039951).  Saving model ...\n",
      "Saved model at epoch 12\n",
      "Epoch: 013, Train Loss: 0.0024241, Valid Loss: 0.0390753, LR: 0.0001000\n",
      "Validation loss decreased (0.039951 --> 0.039075).  Saving model ...\n",
      "Saved model at epoch 13\n",
      "Epoch: 014, Train Loss: 0.0023614, Valid Loss: 0.0379294, LR: 0.0001000\n",
      "Validation loss decreased (0.039075 --> 0.037929).  Saving model ...\n",
      "Saved model at epoch 14\n",
      "Epoch: 015, Train Loss: 0.0023178, Valid Loss: 0.0378608, LR: 0.0001000\n",
      "Validation loss decreased (0.037929 --> 0.037861).  Saving model ...\n",
      "Saved model at epoch 15\n",
      "Epoch: 016, Train Loss: 0.0022876, Valid Loss: 0.0369353, LR: 0.0001000\n",
      "Validation loss decreased (0.037861 --> 0.036935).  Saving model ...\n",
      "Saved model at epoch 16\n",
      "Epoch: 017, Train Loss: 0.0022461, Valid Loss: 0.0364698, LR: 0.0001000\n",
      "Validation loss decreased (0.036935 --> 0.036470).  Saving model ...\n",
      "Saved model at epoch 17\n",
      "Epoch: 018, Train Loss: 0.0022222, Valid Loss: 0.0362586, LR: 0.0001000\n",
      "Validation loss decreased (0.036470 --> 0.036259).  Saving model ...\n",
      "Saved model at epoch 18\n",
      "Epoch: 019, Train Loss: 0.0022013, Valid Loss: 0.0357050, LR: 0.0001000\n",
      "Validation loss decreased (0.036259 --> 0.035705).  Saving model ...\n",
      "Saved model at epoch 19\n",
      "Epoch: 020, Train Loss: 0.0021646, Valid Loss: 0.0350161, LR: 0.0001000\n",
      "Validation loss decreased (0.035705 --> 0.035016).  Saving model ...\n",
      "Saved model at epoch 20\n",
      "Epoch: 021, Train Loss: 0.0021400, Valid Loss: 0.0348801, LR: 0.0001000\n",
      "Validation loss decreased (0.035016 --> 0.034880).  Saving model ...\n",
      "Saved model at epoch 21\n",
      "Epoch: 022, Train Loss: 0.0021173, Valid Loss: 0.0343485, LR: 0.0001000\n",
      "Validation loss decreased (0.034880 --> 0.034349).  Saving model ...\n",
      "Saved model at epoch 22\n",
      "Epoch: 023, Train Loss: 0.0020802, Valid Loss: 0.0342449, LR: 0.0001000\n",
      "Validation loss decreased (0.034349 --> 0.034245).  Saving model ...\n",
      "Saved model at epoch 23\n",
      "Epoch: 024, Train Loss: 0.0020783, Valid Loss: 0.0340005, LR: 0.0001000\n",
      "Validation loss decreased (0.034245 --> 0.034000).  Saving model ...\n",
      "Saved model at epoch 24\n",
      "Epoch: 025, Train Loss: 0.0020490, Valid Loss: 0.0336025, LR: 0.0001000\n",
      "Validation loss decreased (0.034000 --> 0.033602).  Saving model ...\n",
      "Saved model at epoch 25\n",
      "Epoch: 026, Train Loss: 0.0020327, Valid Loss: 0.0342119, LR: 0.0001000\n",
      "Epoch: 027, Train Loss: 0.0020264, Valid Loss: 0.0338084, LR: 0.0001000\n",
      "Epoch: 028, Train Loss: 0.0020020, Valid Loss: 0.0334630, LR: 0.0001000\n",
      "Validation loss decreased (0.033602 --> 0.033463).  Saving model ...\n",
      "Saved model at epoch 28\n",
      "Epoch: 029, Train Loss: 0.0019802, Valid Loss: 0.0334496, LR: 0.0001000\n",
      "Validation loss decreased (0.033463 --> 0.033450).  Saving model ...\n",
      "Saved model at epoch 29\n",
      "Epoch: 030, Train Loss: 0.0019777, Valid Loss: 0.0331626, LR: 0.0001000\n",
      "Validation loss decreased (0.033450 --> 0.033163).  Saving model ...\n",
      "Saved model at epoch 30\n",
      "Epoch: 031, Train Loss: 0.0019538, Valid Loss: 0.0327950, LR: 0.0001000\n",
      "Validation loss decreased (0.033163 --> 0.032795).  Saving model ...\n",
      "Saved model at epoch 31\n",
      "Epoch: 032, Train Loss: 0.0019348, Valid Loss: 0.0329602, LR: 0.0001000\n",
      "Epoch: 033, Train Loss: 0.0019369, Valid Loss: 0.0337839, LR: 0.0001000\n",
      "Epoch: 034, Train Loss: 0.0019423, Valid Loss: 0.0324639, LR: 0.0001000\n",
      "Validation loss decreased (0.032795 --> 0.032464).  Saving model ...\n",
      "Saved model at epoch 34\n",
      "Epoch: 035, Train Loss: 0.0019161, Valid Loss: 0.0324325, LR: 0.0001000\n",
      "Validation loss decreased (0.032464 --> 0.032433).  Saving model ...\n",
      "Saved model at epoch 35\n",
      "Epoch: 036, Train Loss: 0.0018819, Valid Loss: 0.0323984, LR: 0.0001000\n",
      "Validation loss decreased (0.032433 --> 0.032398).  Saving model ...\n",
      "Saved model at epoch 36\n",
      "Epoch: 037, Train Loss: 0.0018669, Valid Loss: 0.0320076, LR: 0.0001000\n",
      "Validation loss decreased (0.032398 --> 0.032008).  Saving model ...\n",
      "Saved model at epoch 37\n",
      "Epoch: 038, Train Loss: 0.0018530, Valid Loss: 0.0319161, LR: 0.0001000\n",
      "Validation loss decreased (0.032008 --> 0.031916).  Saving model ...\n",
      "Saved model at epoch 38\n",
      "Epoch: 039, Train Loss: 0.0018474, Valid Loss: 0.0321468, LR: 0.0001000\n",
      "Epoch: 040, Train Loss: 0.0018340, Valid Loss: 0.0317446, LR: 0.0001000\n",
      "Validation loss decreased (0.031916 --> 0.031745).  Saving model ...\n",
      "Saved model at epoch 40\n",
      "Epoch: 041, Train Loss: 0.0018205, Valid Loss: 0.0317484, LR: 0.0001000\n",
      "Epoch: 042, Train Loss: 0.0018251, Valid Loss: 0.0319154, LR: 0.0001000\n",
      "Epoch: 043, Train Loss: 0.0018229, Valid Loss: 0.0320351, LR: 0.0001000\n",
      "Epoch: 044, Train Loss: 0.0018033, Valid Loss: 0.0315444, LR: 0.0001000\n",
      "Validation loss decreased (0.031745 --> 0.031544).  Saving model ...\n",
      "Saved model at epoch 44\n",
      "Epoch: 045, Train Loss: 0.0018078, Valid Loss: 0.0325703, LR: 0.0001000\n",
      "Epoch: 046, Train Loss: 0.0018341, Valid Loss: 0.0317232, LR: 0.0001000\n",
      "Epoch: 047, Train Loss: 0.0017999, Valid Loss: 0.0317072, LR: 0.0001000\n",
      "Epoch: 048, Train Loss: 0.0017768, Valid Loss: 0.0316699, LR: 0.0001000\n",
      "Epoch: 049, Train Loss: 0.0017724, Valid Loss: 0.0316373, LR: 0.0001000\n",
      "Epoch: 050, Train Loss: 0.0017620, Valid Loss: 0.0314871, LR: 0.0001000\n",
      "Validation loss decreased (0.031544 --> 0.031487).  Saving model ...\n",
      "Saved model at epoch 50\n",
      "Epoch: 051, Train Loss: 0.0017190, Valid Loss: 0.0312282, LR: 0.0000500\n",
      "Validation loss decreased (0.031487 --> 0.031228).  Saving model ...\n",
      "Saved model at epoch 51\n",
      "Epoch: 052, Train Loss: 0.0017119, Valid Loss: 0.0312871, LR: 0.0000500\n",
      "Epoch: 053, Train Loss: 0.0017047, Valid Loss: 0.0311855, LR: 0.0000500\n",
      "Validation loss decreased (0.031228 --> 0.031186).  Saving model ...\n",
      "Saved model at epoch 53\n",
      "Epoch: 054, Train Loss: 0.0016972, Valid Loss: 0.0311295, LR: 0.0000500\n",
      "Validation loss decreased (0.031186 --> 0.031130).  Saving model ...\n",
      "Saved model at epoch 54\n",
      "Epoch: 055, Train Loss: 0.0016840, Valid Loss: 0.0310209, LR: 0.0000500\n",
      "Validation loss decreased (0.031130 --> 0.031021).  Saving model ...\n",
      "Saved model at epoch 55\n",
      "Epoch: 056, Train Loss: 0.0016790, Valid Loss: 0.0310656, LR: 0.0000500\n",
      "Epoch: 057, Train Loss: 0.0016780, Valid Loss: 0.0312140, LR: 0.0000500\n",
      "Epoch: 058, Train Loss: 0.0016753, Valid Loss: 0.0312069, LR: 0.0000500\n",
      "Epoch: 059, Train Loss: 0.0016768, Valid Loss: 0.0313599, LR: 0.0000500\n",
      "Epoch: 060, Train Loss: 0.0016702, Valid Loss: 0.0309856, LR: 0.0000500\n",
      "Validation loss decreased (0.031021 --> 0.030986).  Saving model ...\n",
      "Saved model at epoch 60\n",
      "Epoch: 061, Train Loss: 0.0016557, Valid Loss: 0.0310581, LR: 0.0000500\n",
      "Epoch: 062, Train Loss: 0.0016476, Valid Loss: 0.0308938, LR: 0.0000500\n",
      "Validation loss decreased (0.030986 --> 0.030894).  Saving model ...\n",
      "Saved model at epoch 62\n",
      "Epoch: 063, Train Loss: 0.0016412, Valid Loss: 0.0310533, LR: 0.0000500\n",
      "Epoch: 064, Train Loss: 0.0016381, Valid Loss: 0.0308494, LR: 0.0000500\n",
      "Validation loss decreased (0.030894 --> 0.030849).  Saving model ...\n",
      "Saved model at epoch 64\n",
      "Epoch: 065, Train Loss: 0.0016291, Valid Loss: 0.0309130, LR: 0.0000500\n",
      "Epoch: 066, Train Loss: 0.0016214, Valid Loss: 0.0308885, LR: 0.0000500\n",
      "Epoch: 067, Train Loss: 0.0016224, Valid Loss: 0.0309734, LR: 0.0000500\n",
      "Epoch: 068, Train Loss: 0.0016192, Valid Loss: 0.0309623, LR: 0.0000500\n",
      "Epoch: 069, Train Loss: 0.0016160, Valid Loss: 0.0311327, LR: 0.0000500\n",
      "Epoch: 070, Train Loss: 0.0016095, Valid Loss: 0.0309098, LR: 0.0000500\n",
      "Epoch: 071, Train Loss: 0.0016041, Valid Loss: 0.0309489, LR: 0.0000500\n",
      "Epoch: 072, Train Loss: 0.0016024, Valid Loss: 0.0310968, LR: 0.0000500\n",
      "Epoch: 073, Train Loss: 0.0015967, Valid Loss: 0.0310617, LR: 0.0000500\n",
      "Epoch: 074, Train Loss: 0.0015949, Valid Loss: 0.0308486, LR: 0.0000500\n",
      "Validation loss decreased (0.030849 --> 0.030849).  Saving model ...\n",
      "Saved model at epoch 74\n",
      "Epoch: 075, Train Loss: 0.0015835, Valid Loss: 0.0309899, LR: 0.0000500\n",
      "Epoch: 076, Train Loss: 0.0015756, Valid Loss: 0.0310159, LR: 0.0000500\n",
      "Epoch: 077, Train Loss: 0.0015750, Valid Loss: 0.0311229, LR: 0.0000500\n",
      "Epoch: 078, Train Loss: 0.0015839, Valid Loss: 0.0308760, LR: 0.0000500\n",
      "Epoch: 079, Train Loss: 0.0015723, Valid Loss: 0.0312364, LR: 0.0000500\n",
      "Epoch: 080, Train Loss: 0.0015672, Valid Loss: 0.0311198, LR: 0.0000500\n",
      "Epoch: 081, Train Loss: 0.0015455, Valid Loss: 0.0309960, LR: 0.0000500\n",
      "Epoch: 082, Train Loss: 0.0015424, Valid Loss: 0.0309526, LR: 0.0000500\n",
      "Epoch: 083, Train Loss: 0.0015316, Valid Loss: 0.0309421, LR: 0.0000500\n",
      "Epoch: 084, Train Loss: 0.0015251, Valid Loss: 0.0308852, LR: 0.0000500\n",
      "Epoch: 085, Train Loss: 0.0015191, Valid Loss: 0.0308729, LR: 0.0000500\n",
      "Epoch: 086, Train Loss: 0.0015190, Valid Loss: 0.0309520, LR: 0.0000500\n",
      "Epoch: 087, Train Loss: 0.0015120, Valid Loss: 0.0310167, LR: 0.0000500\n",
      "Epoch: 088, Train Loss: 0.0015179, Valid Loss: 0.0310086, LR: 0.0000500\n",
      "Epoch: 089, Train Loss: 0.0015183, Valid Loss: 0.0309366, LR: 0.0000500\n",
      "Epoch: 090, Train Loss: 0.0015024, Valid Loss: 0.0311430, LR: 0.0000500\n",
      "Epoch: 091, Train Loss: 0.0014993, Valid Loss: 0.0311179, LR: 0.0000500\n",
      "Epoch: 092, Train Loss: 0.0014885, Valid Loss: 0.0309800, LR: 0.0000500\n",
      "Epoch: 093, Train Loss: 0.0014850, Valid Loss: 0.0309399, LR: 0.0000500\n",
      "Epoch: 094, Train Loss: 0.0014774, Valid Loss: 0.0310991, LR: 0.0000500\n",
      "Epoch: 095, Train Loss: 0.0014773, Valid Loss: 0.0312550, LR: 0.0000500\n",
      "Epoch: 096, Train Loss: 0.0014779, Valid Loss: 0.0311915, LR: 0.0000500\n",
      "Epoch: 097, Train Loss: 0.0014763, Valid Loss: 0.0311907, LR: 0.0000500\n",
      "Epoch: 098, Train Loss: 0.0014709, Valid Loss: 0.0311414, LR: 0.0000500\n",
      "Epoch: 099, Train Loss: 0.0014564, Valid Loss: 0.0310000, LR: 0.0000500\n",
      "Epoch: 100, Train Loss: 0.0014451, Valid Loss: 0.0310415, LR: 0.0000500\n",
      "Epoch: 101, Train Loss: 0.0014305, Valid Loss: 0.0309765, LR: 0.0000250\n",
      "Epoch: 102, Train Loss: 0.0014216, Valid Loss: 0.0309814, LR: 0.0000250\n",
      "Epoch: 103, Train Loss: 0.0014173, Valid Loss: 0.0310313, LR: 0.0000250\n",
      "Epoch: 104, Train Loss: 0.0014151, Valid Loss: 0.0309733, LR: 0.0000250\n",
      "Epoch: 105, Train Loss: 0.0014129, Valid Loss: 0.0310620, LR: 0.0000250\n",
      "Epoch: 106, Train Loss: 0.0014099, Valid Loss: 0.0310478, LR: 0.0000250\n",
      "Epoch: 107, Train Loss: 0.0014083, Valid Loss: 0.0309939, LR: 0.0000250\n",
      "Epoch: 108, Train Loss: 0.0014020, Valid Loss: 0.0310396, LR: 0.0000250\n",
      "Epoch: 109, Train Loss: 0.0013970, Valid Loss: 0.0310447, LR: 0.0000250\n",
      "Epoch: 110, Train Loss: 0.0013963, Valid Loss: 0.0310469, LR: 0.0000250\n",
      "Epoch: 111, Train Loss: 0.0013924, Valid Loss: 0.0310270, LR: 0.0000250\n",
      "Epoch: 112, Train Loss: 0.0013894, Valid Loss: 0.0310578, LR: 0.0000250\n",
      "Epoch: 113, Train Loss: 0.0013865, Valid Loss: 0.0310193, LR: 0.0000250\n",
      "Epoch: 114, Train Loss: 0.0013873, Valid Loss: 0.0311093, LR: 0.0000250\n",
      "Epoch: 115, Train Loss: 0.0013811, Valid Loss: 0.0310883, LR: 0.0000250\n",
      "Epoch: 116, Train Loss: 0.0013825, Valid Loss: 0.0311276, LR: 0.0000250\n",
      "Epoch: 117, Train Loss: 0.0013777, Valid Loss: 0.0311630, LR: 0.0000250\n",
      "Epoch: 118, Train Loss: 0.0013735, Valid Loss: 0.0311449, LR: 0.0000250\n",
      "Epoch: 119, Train Loss: 0.0013691, Valid Loss: 0.0311122, LR: 0.0000250\n",
      "Epoch: 120, Train Loss: 0.0013661, Valid Loss: 0.0311430, LR: 0.0000250\n",
      "Epoch: 121, Train Loss: 0.0013625, Valid Loss: 0.0311290, LR: 0.0000250\n",
      "Epoch: 122, Train Loss: 0.0013596, Valid Loss: 0.0311105, LR: 0.0000250\n",
      "Epoch: 123, Train Loss: 0.0013538, Valid Loss: 0.0311447, LR: 0.0000250\n",
      "Epoch: 124, Train Loss: 0.0013518, Valid Loss: 0.0312673, LR: 0.0000250\n",
      "Epoch: 125, Train Loss: 0.0013525, Valid Loss: 0.0312419, LR: 0.0000250\n",
      "Epoch: 126, Train Loss: 0.0013520, Valid Loss: 0.0311927, LR: 0.0000250\n",
      "Epoch: 127, Train Loss: 0.0013473, Valid Loss: 0.0312399, LR: 0.0000250\n",
      "Epoch: 128, Train Loss: 0.0013459, Valid Loss: 0.0312834, LR: 0.0000250\n",
      "Epoch: 129, Train Loss: 0.0013409, Valid Loss: 0.0312404, LR: 0.0000250\n",
      "Epoch: 130, Train Loss: 0.0013377, Valid Loss: 0.0313504, LR: 0.0000250\n",
      "Epoch: 131, Train Loss: 0.0013373, Valid Loss: 0.0312132, LR: 0.0000250\n",
      "Epoch: 132, Train Loss: 0.0013314, Valid Loss: 0.0312649, LR: 0.0000250\n",
      "Epoch: 133, Train Loss: 0.0013287, Valid Loss: 0.0312605, LR: 0.0000250\n",
      "Epoch: 134, Train Loss: 0.0013250, Valid Loss: 0.0312774, LR: 0.0000250\n",
      "Epoch: 135, Train Loss: 0.0013203, Valid Loss: 0.0312810, LR: 0.0000250\n",
      "Epoch: 136, Train Loss: 0.0013166, Valid Loss: 0.0312847, LR: 0.0000250\n",
      "Epoch: 137, Train Loss: 0.0013151, Valid Loss: 0.0312658, LR: 0.0000250\n",
      "Epoch: 138, Train Loss: 0.0013123, Valid Loss: 0.0313586, LR: 0.0000250\n",
      "Epoch: 139, Train Loss: 0.0013104, Valid Loss: 0.0312668, LR: 0.0000250\n",
      "Epoch: 140, Train Loss: 0.0013035, Valid Loss: 0.0313655, LR: 0.0000250\n",
      "Epoch: 141, Train Loss: 0.0013002, Valid Loss: 0.0313221, LR: 0.0000250\n",
      "Epoch: 142, Train Loss: 0.0012977, Valid Loss: 0.0313953, LR: 0.0000250\n",
      "Epoch: 143, Train Loss: 0.0012962, Valid Loss: 0.0313004, LR: 0.0000250\n",
      "Epoch: 144, Train Loss: 0.0012926, Valid Loss: 0.0314006, LR: 0.0000250\n",
      "Epoch: 145, Train Loss: 0.0012902, Valid Loss: 0.0314637, LR: 0.0000250\n",
      "Epoch: 146, Train Loss: 0.0012883, Valid Loss: 0.0314540, LR: 0.0000250\n",
      "Epoch: 147, Train Loss: 0.0012862, Valid Loss: 0.0314802, LR: 0.0000250\n",
      "Epoch: 148, Train Loss: 0.0012815, Valid Loss: 0.0315064, LR: 0.0000250\n",
      "Epoch: 149, Train Loss: 0.0012827, Valid Loss: 0.0314606, LR: 0.0000250\n",
      "Epoch: 150, Train Loss: 0.0012783, Valid Loss: 0.0314931, LR: 0.0000250\n",
      "Epoch: 151, Train Loss: 0.0012700, Valid Loss: 0.0313986, LR: 0.0000125\n",
      "Epoch: 152, Train Loss: 0.0012660, Valid Loss: 0.0314303, LR: 0.0000125\n",
      "Epoch: 153, Train Loss: 0.0012629, Valid Loss: 0.0314153, LR: 0.0000125\n",
      "Epoch: 154, Train Loss: 0.0012619, Valid Loss: 0.0314304, LR: 0.0000125\n",
      "Epoch: 155, Train Loss: 0.0012595, Valid Loss: 0.0314246, LR: 0.0000125\n",
      "Epoch: 156, Train Loss: 0.0012603, Valid Loss: 0.0314495, LR: 0.0000125\n",
      "Epoch: 157, Train Loss: 0.0012561, Valid Loss: 0.0314425, LR: 0.0000125\n",
      "Epoch: 158, Train Loss: 0.0012540, Valid Loss: 0.0314555, LR: 0.0000125\n",
      "Epoch: 159, Train Loss: 0.0012521, Valid Loss: 0.0314157, LR: 0.0000125\n",
      "Epoch: 160, Train Loss: 0.0012510, Valid Loss: 0.0314784, LR: 0.0000125\n",
      "Epoch: 161, Train Loss: 0.0012504, Valid Loss: 0.0314753, LR: 0.0000125\n",
      "Epoch: 162, Train Loss: 0.0012498, Valid Loss: 0.0315047, LR: 0.0000125\n",
      "Epoch: 163, Train Loss: 0.0012515, Valid Loss: 0.0314677, LR: 0.0000125\n",
      "Epoch: 164, Train Loss: 0.0012473, Valid Loss: 0.0314956, LR: 0.0000125\n",
      "Epoch: 165, Train Loss: 0.0012440, Valid Loss: 0.0315114, LR: 0.0000125\n",
      "Epoch: 166, Train Loss: 0.0012431, Valid Loss: 0.0314988, LR: 0.0000125\n",
      "Epoch: 167, Train Loss: 0.0012427, Valid Loss: 0.0315327, LR: 0.0000125\n",
      "Epoch: 168, Train Loss: 0.0012406, Valid Loss: 0.0315355, LR: 0.0000125\n",
      "Epoch: 169, Train Loss: 0.0012388, Valid Loss: 0.0315192, LR: 0.0000125\n",
      "Epoch: 170, Train Loss: 0.0012366, Valid Loss: 0.0315126, LR: 0.0000125\n",
      "Epoch: 171, Train Loss: 0.0012343, Valid Loss: 0.0315339, LR: 0.0000125\n",
      "Epoch: 172, Train Loss: 0.0012331, Valid Loss: 0.0315374, LR: 0.0000125\n",
      "Epoch: 173, Train Loss: 0.0012315, Valid Loss: 0.0315466, LR: 0.0000125\n",
      "Epoch: 174, Train Loss: 0.0012317, Valid Loss: 0.0315556, LR: 0.0000125\n",
      "Epoch: 175, Train Loss: 0.0012296, Valid Loss: 0.0315467, LR: 0.0000125\n",
      "Epoch: 176, Train Loss: 0.0012284, Valid Loss: 0.0315583, LR: 0.0000125\n",
      "Epoch: 177, Train Loss: 0.0012278, Valid Loss: 0.0315976, LR: 0.0000125\n",
      "Epoch: 178, Train Loss: 0.0012251, Valid Loss: 0.0315701, LR: 0.0000125\n",
      "Epoch: 179, Train Loss: 0.0012241, Valid Loss: 0.0316104, LR: 0.0000125\n",
      "Epoch: 180, Train Loss: 0.0012214, Valid Loss: 0.0316116, LR: 0.0000125\n",
      "Epoch: 181, Train Loss: 0.0012206, Valid Loss: 0.0316039, LR: 0.0000125\n",
      "Epoch: 182, Train Loss: 0.0012191, Valid Loss: 0.0316290, LR: 0.0000125\n",
      "Epoch: 183, Train Loss: 0.0012184, Valid Loss: 0.0316152, LR: 0.0000125\n",
      "Epoch: 184, Train Loss: 0.0012182, Valid Loss: 0.0316447, LR: 0.0000125\n",
      "Epoch: 185, Train Loss: 0.0012167, Valid Loss: 0.0316285, LR: 0.0000125\n",
      "Epoch: 186, Train Loss: 0.0012136, Valid Loss: 0.0316453, LR: 0.0000125\n",
      "Epoch: 187, Train Loss: 0.0012118, Valid Loss: 0.0316578, LR: 0.0000125\n",
      "Epoch: 188, Train Loss: 0.0012109, Valid Loss: 0.0316325, LR: 0.0000125\n",
      "Epoch: 189, Train Loss: 0.0012099, Valid Loss: 0.0316691, LR: 0.0000125\n",
      "Epoch: 190, Train Loss: 0.0012059, Valid Loss: 0.0316737, LR: 0.0000125\n",
      "Epoch: 191, Train Loss: 0.0012049, Valid Loss: 0.0316246, LR: 0.0000125\n",
      "Epoch: 192, Train Loss: 0.0012050, Valid Loss: 0.0316900, LR: 0.0000125\n",
      "Epoch: 193, Train Loss: 0.0012043, Valid Loss: 0.0317007, LR: 0.0000125\n",
      "Epoch: 194, Train Loss: 0.0012013, Valid Loss: 0.0317108, LR: 0.0000125\n",
      "Epoch: 195, Train Loss: 0.0011994, Valid Loss: 0.0316738, LR: 0.0000125\n",
      "Epoch: 196, Train Loss: 0.0011976, Valid Loss: 0.0317350, LR: 0.0000125\n",
      "Epoch: 197, Train Loss: 0.0011962, Valid Loss: 0.0316916, LR: 0.0000125\n",
      "Epoch: 198, Train Loss: 0.0011951, Valid Loss: 0.0317263, LR: 0.0000125\n",
      "Epoch: 199, Train Loss: 0.0011943, Valid Loss: 0.0317202, LR: 0.0000125\n",
      "Epoch: 200, Train Loss: 0.0011935, Valid Loss: 0.0317794, LR: 0.0000125\n"
     ]
    }
   ],
   "source": [
    "# for deterministic pytorch algorithms, enable reproducibility.\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG']= \":4096:8\"\n",
    "\n",
    "n_list = [32]\n",
    "\n",
    "LOCAL_MODEL_DIR = 'Model/model_GeoFNO2D.pt'\n",
    "LOCAL_MODEL_IPHI_DIR = 'Model/model_iphi_GeoFNO2D.pt'\n",
    "\n",
    "INPUT_CHANNELS = 3\n",
    "OUTPUT_CHANNELS = 1\n",
    "\n",
    "BETA1 = config['BETA1']\n",
    "BETA2 = config['BETA2']\n",
    "\n",
    "EPOCHS = config['EPOCHS']\n",
    "BATCH_SIZE = config['batch_size']\n",
    "\n",
    "for i in range(len(n_list)):\n",
    "\n",
    "    min_valid_loss = np.inf\n",
    "\n",
    "    width = n_list[i]\n",
    "\n",
    "    today = datetime.datetime.now()\n",
    "\n",
    "    model = FNO2D.FNO2d(modes, modes, width=width, in_channels=INPUT_CHANNELS, out_channels=OUTPUT_CHANNELS, s1=s, s2=s).cuda()\n",
    "    model_iphi = FNO2D.IPHI(width=width).cuda()\n",
    "\n",
    "    print(count_params(model), count_params(model_iphi))\n",
    "\n",
    "    params = list(model.parameters()) + list(model_iphi.parameters())\n",
    "    optimizer = AdamW(params, lr=config['LEARNING_RATE'], weight_decay=1e-4)\n",
    "    # optimizer = dadaptation.DAdaptAdam(params, lr=1, log_every=5, betas=(BETA1, BETA2), weight_decay=1e-4, decouple=True)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "\n",
    "    data_list = []\n",
    "    output_list = []\n",
    "\n",
    "    wandb.init(\n",
    "        anonymous='allow', project=PROJECT_NAME, name= today.strftime('%Y%m%d_%H%M'),\n",
    "            config={\n",
    "                \"epochs\": EPOCHS,\n",
    "                 \"optimizer\": 'AdamW',\n",
    "                \"batch_size\": BATCH_SIZE * config['accum_iter'], 'lr': config['LEARNING_RATE'],\n",
    "                'step_size': step_size, 'gamma': gamma,\n",
    "                'width': width,\n",
    "                'modes': modes,\n",
    "                'loss ': 'L2Loss',\n",
    "                'activation func': 'SELU',\n",
    "                'lr decay': 'steplr',\n",
    "                'in_channels': INPUT_CHANNELS, 'out_channels': OUTPUT_CHANNELS,\n",
    "                'architecture': 'GEOFNO2D - IPHI - accum_gradient - mesh features',\n",
    "                }\n",
    "        )\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        \n",
    "\n",
    "        # get current learning rate\n",
    "        curr_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "        train_loss = train(model, DEVICE, train_loader, optimizer, model_iphi, config)\n",
    "\n",
    "        scheduler.step()    \n",
    "\n",
    "        valid_loss, data_list, output_list, damage_list = validate(model, DEVICE, val_loader, model_iphi, config)\n",
    "        print('Epoch: {:03d}, Train Loss: {:.7f}, Valid Loss: {:.7f}, LR: {:.7f}'.format(epoch, train_loss, valid_loss, curr_lr))\n",
    "        # wandb.log({'train_loss': train_loss, 'valid_loss': valid_loss})\n",
    "        train_loss_list.append(train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "\n",
    "        wandb.log({'train_loss': train_loss, 'valid_loss': valid_loss, 'lr': curr_lr}) \n",
    "\n",
    "        if valid_loss < min_valid_loss:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(min_valid_loss, valid_loss))\n",
    "            min_valid_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            torch.save(model.state_dict(), LOCAL_MODEL_DIR)\n",
    "            torch.save(model_iphi.state_dict(), LOCAL_MODEL_IPHI_DIR)\n",
    "            print('Saved model at epoch {}'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODEL_DIR = 'Model/model_GeoFNO2D.pt'\n",
    "LOCAL_MODEL_IPHI_DIR = 'Model/model_iphi_GeoFNO2D.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2h55d0r4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4331bbf24b647aaad17f281387b8a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>██████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss</td><td>█▆▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>1e-05</td></tr><tr><td>train_loss</td><td>0.00119</td></tr><tr><td>valid_loss</td><td>0.03178</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">20230228_2314</strong>: <a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/2h55d0r4\" target=\"_blank\">https://wandb.ai/jyyresearch/FNO2D/runs/2h55d0r4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230228_231456-2h55d0r4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2h55d0r4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Jin Yi/Documents/ArianaPHD/research/neural_operator/wandb/run-20230301_002249-1q69imwn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/1q69imwn\" target=\"_blank\">rich-waterfall-238</a></strong> to <a href=\"https://wandb.ai/jyyresearch/FNO2D\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d3a46b7bb94d52a4d98c8129712492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='11.556 MB of 11.556 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">rich-waterfall-238</strong>: <a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/1q69imwn\" target=\"_blank\">https://wandb.ai/jyyresearch/FNO2D/runs/1q69imwn</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230301_002249-1q69imwn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# version control model\n",
    "run = wandb.init(project=PROJECT_NAME, job_type='version-model', config=config)\n",
    "trained_model_at = wandb.Artifact(\"Geo\"+PROJECT_NAME, type=\"model\", description=\"trained baseline for GeoFNO2D\")\n",
    "trained_model_at.add_file(LOCAL_MODEL_DIR, name='model_GeoFNO2D.pt')\n",
    "trained_model_at.add_file(LOCAL_MODEL_IPHI_DIR, name='model_iphi_GeoFNO2D.pt')\n",
    "run.log_artifact(trained_model_at)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version control model\n",
    "run = wandb.init(project=PROJECT_NAME, job_type=\"inference\", config=config)\n",
    "trained_model_at = run.use_artifact(\"GeoFNO2D:latest\", type=\"model\")\n",
    "model_dir = trained_model_at.download()\n",
    "\n",
    "# load best model\n",
    "model = FNO2D.FNO2d(modes, modes, width=32, in_channels=3, out_channels=1, s1=s, s2=s).cuda()\n",
    "model_iphi = FNO2D.IPHI(width=32).cuda()\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, 'model_GeoFNO2D.pt')))\n",
    "model_iphi.load_state_dict(torch.load(os.path.join(model_dir, 'model_iphi_GeoFNO2D.pt')))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Jin Yi/Documents/ArianaPHD/research/neural_operator/wandb/run-20230224_211929-3nf02m95</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/3nf02m95\" target=\"_blank\">balmy-tree-226</a></strong> to <a href=\"https://wandb.ai/jyyresearch/FNO2D\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fetch coordinates from wandb\n",
    "run = wandb.init(project=PROJECT_NAME, job_type=\"get_coordinates\", config=config)\n",
    "artifact = run.use_artifact('jyyresearch/FNO2D/fracture-damage-coordinates:latest', type='coordinates')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "gc_coordinates = pd.read_csv(os.path.join(artifact_dir, 'gc_out_coord'), sep=\",\", header=None).to_numpy()\n",
    "d_coordinates = pd.read_csv(os.path.join(artifact_dir, 'd_out_coord'), sep=\",\", header=None).to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss, data_list, output_list, damage_list = validate(model, device, val_loader, model_iphi, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1, id2 = 10, 6  # 75 batchs 8 per batch\n",
    "# i = id1*TEST_BATCH_SIZE + id2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that ids are same with different batch sizes\n",
    "# id1 is the nth batch, id2 is the nth sample in the batch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_n, y_n = gc_coordinates[:, 0], gc_coordinates[:, 1]\n",
    "damage_x, damage_y = d_coordinates[:, 0], d_coordinates[:, 1]\n",
    "\n",
    "# first subplot on left\n",
    "plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_n, y_n, c=data_list[id1][id2][:, 2], cmap='jet', s=10, vmin=0, vmax=6)\n",
    "plt.colorbar()\n",
    "plt.title('input gc field')\n",
    "\n",
    "# plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(damage_x, damage_y, c=damage_list[id1][id2][:,2], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('Original d field')\n",
    "\n",
    "# original d\n",
    "plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(damage_x, damage_y, c=damage_list[id1][id2][:,2], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('Original d field')\n",
    "\n",
    "# d\n",
    "# plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(damage_x, damage_y, c=output_list[id1][id2][:, 0], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('output d field')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1, id2 = 4, 8  # 75 batchs 8 per batch\n",
    "# i = id1*TEST_BATCH_SIZE + id2\n",
    "\n",
    "# subplots\n",
    "# gc\n",
    "\n",
    "\n",
    "x_n, y_n = gc_coordinates[:, 0], gc_coordinates[:, 1]\n",
    "damage_x, damage_y = d_coordinates[:, 0], d_coordinates[:, 1]\n",
    "\n",
    "# first subplot on left\n",
    "plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_n, y_n, c=data_list[id1][id2][:, 2], cmap='jet', s=10, vmin=0, vmax=6)\n",
    "plt.colorbar()\n",
    "plt.title('input gc field')\n",
    "\n",
    "# plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(damage_x, damage_y, c=damage_list[id1][id2][:,2], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('Original d field')\n",
    "\n",
    "# original d\n",
    "plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(damage_x, damage_y, c=damage_list[id1][id2][:,2], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('Original d field')\n",
    "\n",
    "# d\n",
    "# plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(damage_x, damage_y, c=output_list[id1][id2][:, 0], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('output d field')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1, id2 = 4, 15  # 75 batchs 8 per batch\n",
    "# i = id1*TEST_BATCH_SIZE + id2\n",
    "\n",
    "# subplots\n",
    "# gc\n",
    "\n",
    "\n",
    "x_n, y_n = gc_coordinates[:, 0], gc_coordinates[:, 1]\n",
    "damage_x, damage_y = d_coordinates[:, 0], d_coordinates[:, 1]\n",
    "\n",
    "# first subplot on left\n",
    "plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_n, y_n, c=data_list[id1][id2][:, 2], cmap='jet', s=10, vmin=0, vmax=6)\n",
    "plt.colorbar()\n",
    "plt.title('input gc field')\n",
    "\n",
    "# plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(damage_x, damage_y, c=damage_list[id1][id2][:,2], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('Original d field')\n",
    "\n",
    "# original d\n",
    "plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(damage_x, damage_y, c=damage_list[id1][id2][:,2], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('Original d field')\n",
    "\n",
    "# d\n",
    "# plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(damage_x, damage_y, c=output_list[id1][id2][:, 0], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('output d field')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1, id2 = 9, 15  # 75 batchs 8 per batch\n",
    "# i = id1*TEST_BATCH_SIZE + id2\n",
    "\n",
    "# subplots\n",
    "# gc\n",
    "\n",
    "\n",
    "x_n, y_n = gc_coordinates[:, 0], gc_coordinates[:, 1]\n",
    "damage_x, damage_y = d_coordinates[:, 0], d_coordinates[:, 1]\n",
    "\n",
    "# first subplot on left\n",
    "plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_n, y_n, c=data_list[id1][id2][:, 2], cmap='jet', s=10, vmin=0, vmax=6)\n",
    "plt.colorbar()\n",
    "plt.title('input gc field')\n",
    "\n",
    "# plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(damage_x, damage_y, c=damage_list[id1][id2][:,2], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('Original d field')\n",
    "\n",
    "# original d\n",
    "plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(damage_x, damage_y, c=damage_list[id1][id2][:,2], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('Original d field')\n",
    "\n",
    "# d\n",
    "# plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(damage_x, damage_y, c=output_list[id1][id2][:, 0], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('output d field')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1, id2 = 2, 10  # 75 batchs 8 per batch\n",
    "# i = id1*TEST_BATCH_SIZE + id2\n",
    "\n",
    "# subplots\n",
    "# gc\n",
    "\n",
    "\n",
    "x_n, y_n = gc_coordinates[:, 0], gc_coordinates[:, 1]\n",
    "damage_x, damage_y = d_coordinates[:, 0], d_coordinates[:, 1]\n",
    "\n",
    "# first subplot on left\n",
    "plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_n, y_n, c=data_list[id1][id2][:, 2], cmap='jet', s=10, vmin=0, vmax=6)\n",
    "plt.colorbar()\n",
    "plt.title('input gc field')\n",
    "\n",
    "# plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(damage_x, damage_y, c=damage_list[id1][id2][:,2], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('Original d field')\n",
    "\n",
    "# original d\n",
    "plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(damage_x, damage_y, c=damage_list[id1][id2][:,2], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('Original d field')\n",
    "\n",
    "# d\n",
    "# plt.figure(figsize=(18, 7.5))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(damage_x, damage_y, c=output_list[id1][id2][:, 0], cmap='jet', s=10)\n",
    "plt.colorbar()\n",
    "plt.title('output d field')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phasefield",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb9419f644557548447ecb0f1119aef2567eb0b9ec92744eeb7ce809f1fc1aa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
