{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinyi12/miniconda3/envs/phasefield/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import dadaptation\n",
    "import random\n",
    "\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from utilities3 import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Adam import Adam\n",
    "from timeit import default_timer\n",
    "from collections import OrderedDict\n",
    "\n",
    "# import local modules from FNO2D.py\n",
    "import FNO2D\n",
    "\n",
    "\n",
    "import wandb\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset into an Artifact in wandb\n",
    "\n",
    "First step is to load the dataset from .csv files, and add it as an artifact. Artifacts are useful for:\n",
    "1) Dataset versioning\n",
    "2) Supports deduplication which minimizes the storage space used when generating versions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_path = \"./Data/gc_samples_filtered.csv\"\n",
    "d_path = \"./Data/d_samples_filtered.csv\"\n",
    "coordinates_path = \"./Data/coordinates_n\"\n",
    "\n",
    "damage_x_path = \"./Data/x_ver\"\n",
    "damage_y_path = \"./Data/y_ver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "gc = pd.read_csv(gc_path, header=None)\n",
    "d = pd.read_csv(d_path, header=None)\n",
    "\n",
    "# coordinates data from csv file\n",
    "coordinates = pd.read_csv(coordinates_path, header=None)\n",
    "\n",
    "# output mesh coordinates\n",
    "damage_x = pd.read_csv(damage_x_path, header=None)\n",
    "damage_y = pd.read_csv(damage_y_path, header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate damage_x and damage_y\n",
    "damage_xy = np.concatenate((damage_x, damage_y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create numpy array with shape (len(gc), gc.shape[1], 2)\n",
    "input_mesh = np.zeros((len(gc), gc.shape[1], 2))\n",
    "damage_mesh = np.zeros((len(gc), damage_xy.shape[0], 2))\n",
    "\n",
    "# for each sample in input_mesh, add the coordinates\n",
    "for i in range(len(gc)):\n",
    "    input_mesh[i, :, :] = coordinates\n",
    "    damage_mesh[i, :, :] = damage_xy\n",
    "    \n",
    "# add gc to the last dimension of input_mesh to have shape of (len(gc), gc.shape[1], 3)\n",
    "input_data = np.concatenate((input_mesh, np.expand_dims(gc, axis=2)), axis=2)\n",
    "input_data = torch.from_numpy(input_data).float()\n",
    "\n",
    "damage_data = np.concatenate((damage_mesh, np.expand_dims(d, axis=2)), axis=2)\n",
    "damage_data = torch.from_numpy(damage_data).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjyyresearch\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Jin Yi/Documents/ArianaPHD/research/neural_operator/wandb/run-20230203_122033-2ph44pk7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/2ph44pk7\" target=\"_blank\">virtuous-lantern-7</a></strong> to <a href=\"https://wandb.ai/jyyresearch/FNO2D\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"FNO2D\", entity=\"jyyresearch\", job_type=\"upload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_at = wandb.Artifact(\"fracture-damage-raw-data\", type=\"raw_data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataset to hdf5 format for efficient storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input_data and damage_data to h5py file\n",
    "input_data_h5 = input_data.numpy()\n",
    "damage_data_h5 = damage_data.numpy()\n",
    "\n",
    "# save h5py file\n",
    "with h5py.File(\"Data/input_data.h5\", \"w\") as f:\n",
    "    f.create_dataset(\"input_data\", data=input_data_h5)\n",
    "    \n",
    "with h5py.File(\"Data/damage_data.h5\", \"w\") as f:\n",
    "    f.create_dataset(\"damage_data\", data=damage_data_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ManifestEntry digest: pa0A9QJaDxjKYPrY3FclPg==>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add h5py file to artifact\n",
    "raw_data_at.add_file(\"Data/input_data.h5\", name=\"input_data\")\n",
    "raw_data_at.add_file(\"Data/damage_data.h5\", name=\"damage_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bac0eb836741de93ea1304726190d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">virtuous-lantern-7</strong>: <a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/2ph44pk7\" target=\"_blank\">https://wandb.ai/jyyresearch/FNO2D/runs/2ph44pk7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230203_122033-2ph44pk7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.log_artifact(raw_data_at)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact fracture-damage-raw-data:v3, 423.10MB. 2 files... Done. 0:0:0\n"
     ]
    }
   ],
   "source": [
    "artifact = run.use_artifact('jyyresearch/FNO2D/fracture-damage-raw-data:v3', type='raw_data')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./artifacts/fracture-damage-raw-data:v3/input_data.h5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact_dir + \"/input_data.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = h5py.File(artifact_dir + \"/input_data\", \"r\")\n",
    "damage_data = h5py.File(artifact_dir + \"/damage_data\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'gpu'\n",
    "PROJECT_NAME = 'FNO2D'\n",
    "\n",
    "# Set the random seeds to improve reproducibility by removing stochasticity\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False # Force cuDNN to use a consistent convolution algorithm\n",
    "    torch.backends.cudnn.deterministic = True # Force cuDNN to use deterministic algorithms if available\n",
    "    torch.use_deterministic_algorithms(True) # Force torch to use deterministic algorithms if available\n",
    "\n",
    "set_seeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'train_val_split': [0.80, 0.20], # These must sum to 1.0\n",
    "    'batch_size' : 32, # Num samples to average over for gradient updates\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Jin Yi/Documents/ArianaPHD/research/neural_operator/wandb/run-20230203_123738-1us3b8gd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/1us3b8gd\" target=\"_blank\">cheerful-rat-8</a></strong> to <a href=\"https://wandb.ai/jyyresearch/FNO2D\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431405ccf01b4ddfabf03c5dda537646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">cheerful-rat-8</strong>: <a href=\"https://wandb.ai/jyyresearch/FNO2D/runs/1us3b8gd\" target=\"_blank\">https://wandb.ai/jyyresearch/FNO2D/runs/1us3b8gd</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230203_123738-1us3b8gd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define an initial set of transforms that we think will be useful\n",
    "with wandb.init(project=PROJECT_NAME, job_type='define-transforms', config=config) as run:\n",
    "    transform_dict = OrderedDict()\n",
    "    transform_dict['ToTensor'] = {\n",
    "        'device': DEVICE\n",
    "    }\n",
    "    # Include an operational index to verify the order\n",
    "    for key_idx, key in enumerate(transform_dict.keys()):\n",
    "        transform_dict[key]['order'] = key_idx\n",
    "    # Create an artifact for logging the transforms\n",
    "    data_transform_artifact = wandb.Artifact(\n",
    "        'data-transforms', type='parameters',\n",
    "        description='Data preprocessing functions and parameters.',\n",
    "        metadata=transform_dict) # Optional for viewing on the web app; the data is also stored in the txt file below\n",
    "    # Log the transforms in JSON format\n",
    "    with data_transform_artifact.new_file('transforms.txt') as f:\n",
    "        f.write(json.dumps(transform_dict, indent=4))\n",
    "    run.log_artifact(data_transform_artifact)\n",
    "config.update(transform_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some helper functions for transforming numpy to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert numpy arrays to tensor arrays\n",
    "    \"\"\"\n",
    "    def __init__(self, device=None):\n",
    "        if device is None:\n",
    "            device = \"cpu\"\n",
    "        self.device = device\n",
    "    \n",
    "    def __call__(self, data_tuple):\n",
    "        data, labels = data_tuple\n",
    "        return (torch.from_numpy(data).to(self.device), torch.from_numpy(labels).to(self.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(transform_dict):\n",
    "    \"\"\"\n",
    "    Given a dictionary of transform parameters, return a list of class instances for each transform\n",
    "    Arguments:\n",
    "        transform_dict (OrderedDict) with optional keys:\n",
    "            ToTensor (dict) if present, requires the 'device' key that indicates the PyTorch device\n",
    "    Returns:\n",
    "        composed_transforms (PyTorch composed transform class) containing the requested transform steps in order\n",
    "    \"\"\"\n",
    "    transform_functions = []\n",
    "    for key in transform_dict.keys():\n",
    "        if key=='ToTensor': # Convert array to a PyTorch Tensor\n",
    "            transform_functions.append(ToTensor(\n",
    "                transform_dict[key]['device']\n",
    "            ))\n",
    "        \n",
    "    composed_transforms = transforms.Compose(transform_functions)\n",
    "    return composed_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split of input_data and damage_data. Obtain the respective indices\n",
    "train_val_split = config['train_val_split']\n",
    "train_val_indices = np.split(np.random.permutation(len(input_data)), [int(train_val_split[0]*len(input_data))])\n",
    "\n",
    "# create train and test dataset from input_data and damage_data\n",
    "train_dataset = torch.utils.data.TensorDataset(input_data[train_val_indices[0]], damage_data[train_val_indices[0]])\n",
    "val_dataset = torch.utils.data.TensorDataset(input_data[train_val_indices[1]], damage_data[train_val_indices[1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split_artifact(run, raw_data, train_rows, val_rows, test_rows):\n",
    "    \"\"\"\n",
    "    Creates a w&b artifact that contains a singular reference table (aka a ForeignIndex table).\n",
    "    The ForeignIndex table has a single column that we are naming 'source'.\n",
    "    It contains references to the original table (raw_data_table) for each of the splits.\n",
    "    Arguments:\n",
    "        run (wandb run) returned from wandb.init()\n",
    "        raw_data_table (wandb Table) that contains your original tabular data\n",
    "        train_rows (list of ints) indices that reference the training rows in the raw_data_table\n",
    "        val_rows (list of ints) indices that reference the validation rows in the raw_data_table\n",
    "        test_rows (list of ints) indices that reference the test rows in the raw_data_table\n",
    "    \"\"\"\n",
    "    split_artifact = wandb.Artifact(\n",
    "        'data-splits', type='dataset',\n",
    "        description='Train, validation, test dataset splits')\n",
    "\n",
    "    # Our data split artifact will only store index references to the original dataset to save space\n",
    "    split_artifact.add(wandb.Table(\n",
    "        columns=['source'],\n",
    "        data=train_rows), 'train-data')\n",
    "\n",
    "    split_artifact.add(wandb.Table(\n",
    "        columns=['source'],\n",
    "        data=val_rows), 'val-data')\n",
    "\n",
    "    run.log_artifact(split_artifact)\n",
    "\n",
    "\n",
    "def make_loaders(config):\n",
    "    \"\"\"\n",
    "    Makes data loaders using a artifact containing the dataset splits (created using the make_split_artifact() function)\n",
    "    The function assumes that you have created a data-splits artifact and a data-transforms artifact\n",
    "    Arguments:\n",
    "        config [dict] containing keys:\n",
    "            batch_size (int) amount of rows (i.e. data instances) to be delivered in a single batch\n",
    "    Returns:\n",
    "        train_loader (PyTorch DataLoader) containing the training data\n",
    "        val_loader (PyTorch DataLoader) containing the validation data\n",
    "    \"\"\"\n",
    "    with wandb.init(project=PROJECT_NAME, job_type='package-data', config=config) as run:\n",
    "        # Load transforms\n",
    "        transform_dir = run.use_artifact('data-transforms:latest').download()\n",
    "        transform_dict = json.load(open(os.path.join(transform_dir, 'transforms.txt')), object_pairs_hook=OrderedDict)\n",
    "        composed_transforms = get_transforms(transform_dict)\n",
    "\n",
    "        split_artifact = run.use_artifact('data-splits:latest')\n",
    "        # Reformat data to (inputs, labels)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            batch_sampler=None,\n",
    "            shuffle=False,\n",
    "            num_workers=0)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phasefield",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb9419f644557548447ecb0f1119aef2567eb0b9ec92744eeb7ce809f1fc1aa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
